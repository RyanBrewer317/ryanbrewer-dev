id: simpler-category-theory
title: Simpler Category Theory
date: 2025-06-16
tags: programming,language,formal,monoid,group
blerb: Category theory seems to be all the rage in cutting-edge programming language research and in typed functional programming. This post offers a simple entrypoint.

A lot of advanced concepts in programming language research and functional programming recently has heavily referenced ideas from the field of category theory. This short post hopes to offer an accessible tutorial for those who want to dig deeper into the scary mathy research papers.

### A Detour Through Group Theory

First I'll mention group theory; if you're comfortable with it already you can skip this section. It seems many people have heard of it, and foggy images of Rubik's cubes, clocks, and snowflakes might come to mind. A group is a system for talking about *reversibility* in a mathematical way. It can also be understood in terms of symmetry or things that cancel out. Because it's describing something that shows up almost everywhere, we're going to use a lot of letters, standing in the place of actual real-world things (more on this in a moment). The most fundamental rule of a group is this equation:

@math
a\cdot a^{-1} = e

Here \\(\\cdot\\) intuitively means "and then," and \\(e\\) represents "nothing." Group theory needs this notion of nothingness, because the above equation is saying "Any \\(a\\) followed by its inverse amounts to nothing." That is, \\(a\\) and \\(a^{-1}\\) are two elements in the group that "reverse" each other or "cancel-out." To capture this formally we need a few more rules:

@math
a^{-1}\cdot a = e

@math
a\cdot e = a

@math
e\cdot a = a

@math
a\cdot(b\cdot c)=(a\cdot b)\codt c

We need the second rule because \\(a\\cdot b\\) might not be the same as \\(b\\cdot a\\). So in *general* the order of elements matters, but for cancelling-out it doesn't matter if the inverse is on the left side or the right side of the \\(\\cdot\\). The third and fourth rule just state that \\(e\\) is basically nothing. The fifth rule ("associativity") lets us write \\(a\\cdot b\\cdot c\\), by stating that the various ways to parenthesize it are equal, so we don't need parentheses. With these rules we can prove the following:

@math
a\cdot a^{-1}\cdot b = b

First we replace the \\(a\\) and its inverse with \\(e\\), then we apply the fourth rule to get rid of the \\(e\\), ending up with \\(b\\). A very pleasant proof: this feels a bit like middle-school algebra!

Now, what *are* these letters exactly? We have some sense of \\(e\\) (but not much) and some sense of \\(\\cdot\\) (but not much), and we don't really know what \\(a\\) and \\(b\\) are at all. Well, the classic example uses a Rubik's cube: we can say that \\(a\\) rotates the top horizontal part 90 degrees clockwise, and \\(b\\) rotates the middle horizontal part 90 degrees clockwise. \\(e\\) then would have to be rotating something 0 or 360 degrees (or some multiple of 360 degrees): "doing nothing." \\(\\cdot\\) refers to doing the motions in sequence. We would then use letters \\(c\\), \\(d\\), etc. for all the other 90-degree, 180-degree, and 270-degree clockwise rotations of Rubik's cube sections. Note that one of these (say, \\(n\\)) refers to rotating the top horizontal part 270 degrees clockwise, which perfectly undoes \\(a\\). So we can say \\(n=a^{-1}\\). If we recall the expression \\(a\\cdot a\\^{-1}\\cdot b\\), this refers to rotating the top horizontal part 90 degrees, then 270 degrees, then rotating the middle horizontal part 90 degrees. If you do these steps in this order, the end result is the same as just rotating the middle horizontal part 90 degrees: the whole thing indeed equals \\(b\\)!

Ok, so the letters, the group elements, are Rubik's cube motions. Right? Unfortunately not. Other groups are very different. Take the integers, with addition. We'll say \\(e=0\\), \\(\\cdot=+\\), and \\(a^{-1}=-a\\). Then the letters mean integers: if \\(a=6\\) and \\(b=-32\\), then \\(a\\cdot a\\^{-1}\\cdot b=6+(-6)+(-32)=-32\\). This is another valid group! Group theory wants to talk about *every group at the same time.* Therefore we just use letters for the elements. We also ignore details of the particular groups: Rubik's cubes have the equation \\(a\\cdot a\\cdot a=a^{-1}\\), but our integer group does not, so we can't use this equation in pure group theory. After all, if we depended on this equation in a proof and then tried to use our proof on our integer group, it would likely go quite badly!

I say all of this about letters and group theory to get us comfortable using letters and operations without even *caring* about what they represent. This can be a bump in the learning curve for a lot of people, but it's crucial for getting into category theory!

### Monoid Theory

Now let me introduce a concept that sounds pretty scary: "monoids." Most people who have heard of monoids might think of the term as nothing more than incomprehensible alien-speak, but it's actually a very simple idea. We just get rid of our first two rules for groups! We've still got a "do nothing" element \\(e\\), and it disappears as we expect, but besides that nothing cancels out anymore. That is to say, in *monoid* theory we can't cancel them out; individual monoids can still have their own extra equations, like the Rubik's cube example for group theory. This means that every group is a monoid: anything that satisfies our five rules of group theory clearly satisfy our last three rules of group theory! \\(a\\cdot e\\cdot b=a\\cdot b\\) is something you can prove in monoid theory, and indeed it's true in both of our group examples.

The point of monoid theory is that there are many monoids that aren't groups, and therefore aren't described by group theory. For example, imagine stacks of plates. You can put a stack of three plates on top of a stack of four plates, and you may find you've constructed a stack of seven plates. Putting a stack of zero plates on a stack gives the same stack, as does putting a stack on top of a stack of zero plates. But we're just talking about adding plates, not removing them, so there's no way to undo these actions. Once you've put three plates on top of four plates, there's no amount of plates you can add to get to four plates. This isn't a group, but it is a monoid, and our \\(a\\cdot e\\cdot b=a\\cdot b\\) equation works for it.

### Category Theory

This is a post about category theory, right? To get to category theory we'll need to remove one last thing. So far we've been able to use \\(\\cdot\\) for *any* two elements in the group or monoid. Now let's imagine that our elements are jigsaw puzzle pieces. The left and right have a certain shape, and \\(a\\cdot b\\) is only defined when the right side of \\(a\\) has the matching shape for the left side of \\(b\\). This is a category. As you can imagine, every monoid is a category (one where all edges match) and therefore every group is a category too. 

In category theory we write \\(;\\) instead of \\(\\cdot\\) for the operator, but this is just a cosmetic change. You'll often see \\(\\circ\\) used instead, pronounced "after," which is just \\(;\\) with the sides flipped: \\(a;b=b\\circ a\\). I'm not going to use \\(\\circ\\) here because it brings in unnecessary confusion. We also typically call the elements "morphisms," precisely because it's a meaningless word and therefore doesn't come with any mental connotations.

We also need one more thing to form a category. For every edge-shape, one of the morphisms has to have that edge shape on both the left and the right side. Let's call the edge-shape \\(A\\), and the morphism \\(\\texttt{id}_A\\). This will serve the purpose of the "do nothing" element \\(e\\) in monoid and group theory, so we furthermore need the equations \\(\\texttt{id}_A;a=a;\\texttt{id}_B=a\\) for any morphism \\(a\\) with an \\(A\\)-shaped left edge and a \\(B\\)-shaped right edge. (For these morphisms we write \\(a:A\\to B\\).) Notice that now there isn't just one "do nothing" element, but one for every edge-shape!

Even though the concept of matching edges on jigsaw puzzle pieces is very useful for thinking about category theory, I'm now going to switch to the actual term: "objects." When a category theorist says "a morphism \\(f\\) goes from object \\(A\\) to object \\(B\\)," they mean there's an element \\(f\\) with an \\(A\\)-shaped left edge and a \\(B\\)-shaped right edge, aka \\(f:A\\to B\\). We confusingly call the morphisms "arrows" between objects, with is sometimes helpful but often harmful, losing the nice (and effective!) intuition of middle-school algebra. A lot of the beginner ideas in category theory can be easily understood using "arrows" between "objects," but I've personally found that the ceiling here is surprisingly low. 

### Using Category Theory

In functional programming and PL theory, we often want our objects to be types, as in `int` and `void`. We then have a category of typed functions, like \\(\\texttt{len}:\\texttt{str}\\to\\texttt{int}\\). Our \\(a;b\\) operator is defined to be some function \\(c\\) such that \\(c(x)=b(a(x))\\), that is, applying \\(a\\) to the input and then applying \\(b\\) to that. As you can imagine, for \\(f:A\\to B\\) and \\(g:B\\to C\\), \\(f;g:A\\to C\\), as is the case in any category.

I'm going to call our category of types and functions \\(H\\).

As vaguely interesting as \\(H\\) may be, things get actually useful when we consider *categories of categories.* What we mean when we say this is categories where the *objects* (element-edge-shapes) are categories. A morphism \\(F:C\\to D\\) from a category \\(C\\) to a category \\(D\\) is basically a function that turns every object \\(A\\) in \\(C\\) into an object in \\(D\\) (an object which I'll just refer to as \\(F(A)\\), for the sake of fewer names) and every morphism \\(a:A\\to B\\) in \\(C\\) into a morphism \\(F(a):F(A)\\to F(B)\\) in \\(D\\). We'll also require a few useful equations:

@math
F(f;g)=F(f);F(g)

@math
F(\texttt{id}_A)=\texttt{id}_{F(A)}

That is, these morphisms "preserve" composition and identities. If you're quite familiar with abstract algebra (including group theory) you might expect to call these morphisms "category homomorphisms." Instead we call them "functors." It's a useless and unfortunate name. But with functors, we can form a category where the objects are categories. To avoid paradoxes, this big category is not an object inside itself; I won't get into the details of that here.

If you're familiar with typed functional programming, you're likely perking up at the word "functor." How can it be that these functions between categories, these morphisms in a *category of categories,* are the same as the functors you might be used to in, say, Haskell? The answer is that Haskell functors go from the category of types and functions *back to itself.* Or in other words, they're morphisms in the category of categories that have the category of types and functions on *both* the left and right. The functor \\(\\texttt{Maybe}\\) turns the type \\(\\texttt{Int}\\) into the type \\(\\texttt{Maybe Int}\\), and the morphism \\(\\texttt{plusTwo}:\\texttt{Int}\\to\\texttt{Int}\\) into the morphism \\(\\texttt{map}(\\texttt{plusTwo}):\\texttt{Maybe(Int)}\\to\\texttt{Maybe(Int)}\\).

So our category of categories has already given us a deeper understanding of the structure of our category of types and functions, giving us a richer language of datatypes and functions. Beginners often think of functors as data structures, since lists and trees are functors too, turning a given type \\(\\texttt{T}\\) into the type of a datastructure holding values of type \\(\\texttt{T}\\). Functors are much more broad a concept than just datastructures, however, so be careful to not get trapped in a mental rut when you see a functor that looks nothing like a datastructure!

Lets construct another useful category, called a "product category." I'll write \\(C\\times D\\), where \\(C\\) and \\(D\\) are other categories. The objects here will be *pairs* of objects, the left one being from \\(C\\) and the right one being from \\(D\\). The morphisms will be pair of morphisms \\((f,g):(A,X)\\to(B,Y)\\), where \\(f:A\\to B\\) is a morphism in \\(C\\) and \\(g:X\\to Y\\) is a morphism in \\(D\\). The reason we write \\(C\\times D\\) and call it a "product category" is because the number of morphisms from \\((A,X)\\) to \\((B,Y)\\) is equal to the number of morphisms from \\(A\\) to \\(B\\) in \\(C\\) *times* the number of morphisms from \\(X\\) to \\(Y\\) in \\(D\\). You can imagine drawing out a multiplication table for them!

Composition and identity in a product category works how you'd expect. For \\((f,g):(A,X)\\to(B,Y)\\) and \\((h,k):(B,Y)\\to(Z,W)\\), \\((f,g);(h,k):(A,X)\\to(Z,W)\\) is defined as the pair \\((f;h,g;k)\\). For every object \\((A,B)\\), \\(id_{(A,B)}:(A,B)\\to(A,B)\\) is defined as the pair \\((id_A,id_B)\\).

Whew! That took a bunch of math-speak to define clearly, but hopefully you can see that a product category \\(C\\times D\\) is just pairs of everything (always the left side being from \\(C\\) and the right side being from \\(D\\)). 

This is more powerful than you'd expect! Why? For any two categories in our category-of-categories, their product category is *also* in the category-of-categories! This means we can start using functors to get even more value!

Consider something like `Result<T, U>`, or the equivalent in Haskell, `Either a b`. This is *not* a functor from \\(H\\) to itself. This is a functor from \\(H\\times H\\) to \\(H\\)! Every object \\((A,B)\\) in \\(H\\times H\\) gets turned into the object \\(\\texttt{Result}(A,B)\\) in \\(H\\), and every morphism \\((f,g):(A,X)\\to(B,Y)\\) in \\(H\\times H\\) gets turned into the morphism \\(\\texttt{map}(f,g):\\texttt{Result}(A,X)\\to\\texttt{Result}(B,Y)\\) in \\(H\\). We know this particular morphism either runs \\(f\\) or \\(g\\), depending on if it's a success value or failure value. As you can imagine, this does satisfy the functor rules.