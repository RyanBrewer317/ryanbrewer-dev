id: linear-logic
title: Par Part 2: Linear Logic
date: 2025-02-07
tags: x
blerb: shh, this post isn't really public yet! You came at a special time :)

### The Problem

This post is the second in a series on Logic. These ideas are very useful in understanding many important papers on programming language theory, especially papers on type theory and the lambda calculus. The first post was an explanation of sequents and Sequent Calculus as a system for doing logic, so I'll assume that you have an understanding of those concepts now. In this post I'll dive into linear logic, as the most amazing result of studying the sequent calculus. I'll finish the trilogy in the next post, where I discuss Par and computational interpretations of classical logic.

In the last post we realized that sequent notation can give a fairly elegant characterization of intuitionistic logic: restricting the right side of the turnstile (\\(\\vdash\\)) to having at most one expression. As pleasing as that is, it also costs us a lot of the beauty of the sequent calculus, namely the symmetry between the two sides of the turnstile. Recall that in a classical sequent calculus, \\(p,q\\vdash r,s\\) is equivalent to \\(p,q,\\neg r,\\neg s\\vdash\\) and also \\(\\vdash\\neg p,\\neg q,r,s\\). Let's take a moment to recap why that is, as a warmup for getting comfortable with the notation again:

In a more basic logical notation we could write the first one (informally) as \\((p\\land q)\\Rightarrow (r\\lor s)\\). 

The second one would be \\((p\\land q\\land\\neg r\\land\\neg s)\\Rightarrow\\texttt{False}\\), because an empty right side means there must be a contradiction on the left. If you're familiar with abstract algebra or monoids, this is also because \\(\\texttt{False}\\) is the "unit" for disjunction. So we now have \\(\neg(p\\land q\\land\\neg r\\land\\neg s)\\), which we can De-Morgan into \\(\\neg p\\lor\\neg q\\lor r\\lor s\\). In classical logic, \\(\\neg p\\lor q\\) is equivalent to implication, so let's do a little reverse-De-Morganing to get it into that form: \\(\\neg(p\\land q)\\lor r\\lor s\\). Finally we have \\(p\\land q\\Rightarrow r\\lor s\\). Every step in this translation was reversible, so we know the two forms are equivalent.

The third one follows a very similar process so I won't go in as much depth. \\(\\texttt{True}\\) is the unit for conjunction, so we get \\(\\texttt{True}\\Rightarrow(\\neg p\\lor\\neg q\\lor r\\lor s)\\) and I'm sure you can do the rest.

(If you've done your category theory reading then I'll leave an easter egg as a reward: moving negation across implication like this is because negation is self-[adjoint](/wiki/adjunction), a property that gives rise to the double-negation monad used to model continuations! But all of this is waaay above the expected background knowledge for this series :)

Well now we can see how to move things back and forth between the two sides of the turnstile, which is kinda cool. This process is how we did the one-sided sequent calculus in the last post, which if you'll recall only worked for classical logic. Since intuitionistic logic requires always having at most one expression on the right side, none of that works. And this is related to the fact that negation in intuitionistic logic is kinda wonky, and loses the gorgeous dualities of classical logic.

Having run into fundamental inconveniences with intuitionistic logic, the most natural question to ask is "why do we even care about intuitionistic logic? Why not just use classical logic, with all those nice properties?" The big problem with classical logic is the Law of Excluded Middle, or LEM. This is the axiom that lets us assume \\(p\\lor\\neg p\\) for any proposition \\(p\\) at all. Or equivalently the Double-Negation Elimination law, or DNE, which says that a proof that \\(\\neg p\\) is *false* is enough to prove that \\(p\\) is *true* (\\(\\neg(\\neg p)\\Rightarrow p\\)). Both of these feel true enough, but they don't give any *reason* that \\(p\\) is true, beyond just the fact that it isn't false. In intuitionistic logic, you can look at a proof of something and determine exactly what makes that thing true, from the proof itself. Take the example where \\(p\\) is "this Turing machine halts on this input." The classical logician says, "clearly it either halts on the input or it doesn't, there's no third option, so \\(p\\lor\\neg p\\)." The intuitionist then points out, "you clearly believe that either \\(p\\) or \\(\\neg p\\) is true, so you should be able to tell me which one!" There isn't a way to do this in general, of course: you end up just running the machine on the input, but you can't wait "forever" to see if it halts or not. Intuitionistic logic has a powered-up disjunction: you can only prove a disjunction when you know which side of it is true, so when given a disjunction it's always valid to ask which side of it is true. It's an awesome property that puts intuitionistic logic in correspondence with various mathematical objects like toposes, bounded lattices, and of course the lambda calculus.

Great, so we find ourselves in a situation where both classical logic and intuitionistic logic have some fundamental annoyances. None of them are major problems, and people do serious work in both systems to this day, but if we somehow found a logic that solved all these problems then we'd know it's somehow more "fundamental" and "natural," aka it probably has benefits we can't predict until we find such a logic.

### The Solution

A French logician named Jean-Yves Girard had all of these thoughts, and decided to see if sequent calculus can help. If you want all that nice symmetry across the turnstile, then of course your system needs to be some kind of sequent calculus in the first place. So the trick is defining a sequent calculus with negation-based moves across the turnstile where disjunctions (and existentials) still have that nice property of "remembering" what specifically is true.

(The word for this is "constructive." The idea is the proofs are "constructed" from smaller proofs, never erasing them like classical logic does. Intuitionistic logic is definitely thought of as the "main" constructive logic, causing the two to get conflated. Girard even says on page 4 of his [work](https://doi.org/10.1016/0304-3975\(87\)90045-4) that I'm about to talk about that "There is no constructive logic beyond intuitionistic logic." However, that very work introduces the first serious contender with intuitionistic logic in the space of constructive logics!)

So why is it, exactly, that limiting a classical sequent calculus to only ever have at most one expression on the right makes the system constructive? Girard realized that it's because this prevents any usage of the contraction structural rule on the right side: if you can't have two proofs of \\(p\\lor q\\) on the right side, then you can't use contraction to remove the duplicate. So classical logic can have \\(\\Gamma\\vdash p,q\\), use \\(\\lor\\)-Intro-R\\({}_1\\) and \\(\\lor\\)-Intro-R\\({}_2\\) to get to \\(\\Gamma\\vdash p\\lor q,p\\lor q\\), and contraction to dispose of either one of the two, ending up with \\(\\Gamma\\vdash p\\lor q\\). We could have disposed of either disjunction, so even if we classically know that this disjunction is true, any *evidence* that it's true (via the right or left side) has been completely forgotten.

So Gentzen proposes that, if the goal of the at-most-one-expression restriction on the right is to disallow contraction in instances that would break constructivity, then we can remove the restriction by instead simply disallowing contraction completely. Now we have the nice symmetry once more, and in fact Girard presents his new logic in the style of the one-sided sequent I mentioned in the last post! Now, a new problem arises in the face of constructivity, however, which is that you can simply using weakening to introduce a disjunction, obviously without knowing which side is true (the disjunction doesn't even need to be true!). So Girard simply bans weakening as well. (This wasn't a problem before because right-weakening was only possible when the right was empty, which already means there's a contradiction on the left.) In Girard's own words, "Once Once we have recognized that the constructive features of intuitionistic logic come from the dumping of structural rules on a specific place in the sequents, we are ready to face the consequences of this remark: the limitation should be generalized to the other rooms, i.e., weakening and contraction disappear" ([Girard 1987](https://doi.org/10.1016/0304-3975\(87\)90045-4), p. 4).

This final system, classical logic minus weakening and contraction, completely constructive, is called Linear Logic. The main downside now is that, well, there's no weakening and there's no contraction. Expressions can't be duplicated or dropped, so everything must be used exactly once. Girard has a solution to this that I'll get to later. One really weird consequence of this is that linear logic has a notion of truth/validity that isn't "monotonic," meaning that the amount of information you have to work with doesn't just go up throughout the proof. A proposition you've proven become "unproven" once you use it, meaning you need a new proof of it to use it again.

### The Operators

Unfortunately, one does not simply remove weakening and contraction. With them gone, there are multiple ways to interpret the introduction and elimination rules from the one-sided sequent I presented in the last post. Take the example of \\(\\land\\)-Intro:

@math
\begin{prooftree}
\AxiomC{$\vdash p,\Delta$}
\AxiomC{$\vdash q,\Delta$}
\RightLabel{$\land$-Intro}
\BinaryInfC{$\vdash p\land q,\Delta$}
\end{prooftree}

When we bring this from classical logic to linear logic, we have to look at that \\(\\Delta\\) with new caution. Taken directly into linear logic, this rule says that the \\(p\\) and the \\(q\\) have to be in the exact collection of propositions in order to form a conjunction with them. This is a big ask in linear logic, now that we can't introduce or remove propositions to make the two collections match up! Perhaps we prefer the following rule instead? It's equivalent in classical logic, but very much not in linear logic.

@math
\begin{prooftree}
\AxiomC{$\vdash p,\Gamma$}
\AxiomC{$\vdash q,\Delta$}
\RightLabel{$\land$-Intro}
\BinaryInfC{$\vdash p\land q,\Gamma,\Delta$}
\end{prooftree}

This states what might feel a little more intuitive, namely that conjunction doesn't actually care at all about the rest of the collections. But it's limiting in its own way, because now, if \\(\\Gamma\\) and \\(\\Delta\\) *are* equal, then the conjunction is in a collection where everything is duplicated! With no ability to remove them, this can cause serious problems. For example, we can't even prove \\(p\\Rightarrow p\\land p\\), aka \\(\\vdash\\neg p,p\\land p\\):

@math
\begin{prooftree}
\AxiomC{}
\RightLabel{Axiom}
\UnaryInfC{$\vdash\neg p,p$}
\AxiomC{}
\RightLabel{Axiom}
\UnaryInfC{$\vdash\neg p,p$}
\RightLabel{$\land$-Intro}
\BinaryInfC{$\vdash\neg p,\neg p,p\land p$}
\end{prooftree}

You can hopefully see that there's this \\(\\neg p\\) left over at the bottom and no way to get rid of it. Or equivalently, we've proven \\(p\\Rightarrow(p\\Rightarrow(p\\land p))\\), but not our goal.

Linear logic simply decides to keep both! \\(\\land\\) ("and") disappears, replaced by \\(\\&\\) ("with") and \\(\\otimes\\) ("tensor"):

@math
\begin{prooftree}
\AxiomC{$\vdash p,\Delta$}
\AxiomC{$\vdash q,\Delta$}
\RightLabel{$\&$-Intro}
\BinaryInfC{$\vdash p\&q,\Delta$}
\end{prooftree}

@math
\begin{prooftree}
\AxiomC{$\vdash p,\Gamma$}
\AxiomC{$\vdash q,\Delta$}
\RightLabel{$\otimes$-Intro}
\BinaryInfC{$\vdash p\otimes q,\Gamma,\Delta$}
\end{prooftree}

So now \\(p\\Rightarrow p\\&p\\) is a theorem but \\(p\\Rightarrow p\\otimes p\\) is not. Meanwhile, because \\(\\&\\) *does* get rid of that extra \\(\\neg p\\), \\(p\\Rightarrow(p\\Rightarrow(p\\&p))\\) is *not* a theorem, but of course \\(p\\Rightarrow(p\\Rightarrow(p\\otimes p))\\) is.

We do all this same stuff with disjunction, because we also have two possible translations of the classical and intuitionistic \\(\\lor\\) rules.